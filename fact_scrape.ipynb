{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>JUDGMENT:</strong>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/430 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [01:08<00:00,  6.27it/s]\n",
      "100%|██████████| 430/430 [00:59<00:00,  7.28it/s]\n",
      "100%|██████████| 430/430 [01:06<00:00,  6.43it/s]\n",
      "100%|██████████| 430/430 [01:00<00:00,  7.08it/s]\n",
      "100%|██████████| 430/430 [01:05<00:00,  6.55it/s]\n",
      "100%|██████████| 430/430 [01:00<00:00,  7.07it/s]\n",
      "100%|██████████| 430/430 [01:16<00:00,  5.59it/s]\n",
      "100%|██████████| 430/430 [00:59<00:00,  7.25it/s]\n",
      "100%|██████████| 430/430 [01:11<00:00,  6.05it/s]\n",
      "100%|██████████| 430/430 [00:58<00:00,  7.37it/s]\n",
      "100%|██████████| 430/430 [01:11<00:00,  6.00it/s]\n",
      "100%|██████████| 430/430 [01:08<00:00,  6.29it/s]\n",
      "100%|██████████| 430/430 [01:06<00:00,  6.49it/s]\n",
      "100%|██████████| 430/430 [01:06<00:00,  6.51it/s]\n",
      "100%|██████████| 430/430 [01:10<00:00,  6.12it/s]\n",
      "100%|██████████| 430/430 [01:03<00:00,  6.72it/s]\n",
      "100%|██████████| 430/430 [01:16<00:00,  5.65it/s]\n",
      "100%|██████████| 430/430 [01:04<00:00,  6.70it/s]\n",
      "100%|██████████| 430/430 [05:23<00:00,  1.33it/s]  \n",
      "100%|██████████| 430/430 [02:05<00:00,  3.41it/s] \n",
      "100%|██████████| 430/430 [01:35<00:00,  4.50it/s]\n",
      "100%|██████████| 430/430 [01:49<00:00,  3.92it/s]\n",
      "100%|██████████| 430/430 [02:40<00:00,  2.67it/s]\n",
      "100%|██████████| 430/430 [01:47<00:00,  4.00it/s]\n",
      "100%|██████████| 430/430 [03:57<00:00,  1.81it/s]\n",
      "100%|██████████| 430/430 [02:48<00:00,  2.55it/s] \n",
      "100%|██████████| 430/430 [01:45<00:00,  4.08it/s]\n",
      "100%|██████████| 430/430 [00:57<00:00,  7.44it/s]\n",
      "100%|██████████| 430/430 [05:57<00:00,  1.20it/s]\n",
      "100%|██████████| 430/430 [01:56<00:00,  3.68it/s]\n",
      "100%|██████████| 430/430 [07:48<00:00,  1.09s/it]  \n",
      "100%|██████████| 430/430 [02:18<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.elitigation.sg/gd/gd/{year}_{court}_{case_id}'\n",
    "    \n",
    "years = range(2000, 2016)\n",
    "courts = ['SGHC', 'SGCA']\n",
    "case_ids = range(0, 430)\n",
    "\n",
    "data = []\n",
    "fact_issues_identifiers = ['Background facts', 'The facts', 'Background', 'Facts', 'Finding of Fact', 'The issues', \n",
    "                           'The appeal', 'Appeal', 'The issues on appeal', 'The claim', 'The present claim', \n",
    "                           'Background to the dispute', 'The substantive issue', 'Issues to be determined', 'The relevant issues']\n",
    "\n",
    "# Iterate over all combinations of years, courts, and case_ids\n",
    "for year in years:\n",
    "    for court in courts:\n",
    "        for case_id in tqdm(case_ids):\n",
    "            url = base_url.format(year=year, court=court, case_id=case_id)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            extracted_text = ''\n",
    "\n",
    "            # Parse the HTML content of the webpage\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all <strong> tags\n",
    "            strong_tags = soup.find_all('strong')\n",
    "            #print(strong_tags)\n",
    "\n",
    "            if response.status_code == 200: # Elitigation page returns status code 200 even for 404\n",
    "                # Find all <h1> tags\n",
    "                h1_tags = soup.find_all('h1')\n",
    "                \n",
    "                # Check if any of the <h1> tags contain the text 'Page Not Found'\n",
    "                page_not_found = any(tag.get_text().strip() == 'Page Not Found' for tag in h1_tags)\n",
    "\n",
    "                if not page_not_found:\n",
    "                    #print(strong_tags)\n",
    "                    \n",
    "                    # Iterate through all <strong> tags\n",
    "                    for strong_tag in strong_tags:\n",
    "                        text = strong_tag.get_text()\n",
    "                        # Check if keywords of facts and issues is in the text\n",
    "                        if [identifier for identifier in fact_issues_identifiers if (identifier in text)]:\n",
    "                            # Find the parent <p> tag of the current <strong> tag\n",
    "                            parent_p_tag = strong_tag.find_parent('p')\n",
    "\n",
    "                            #print(parent_p_tag)\n",
    "                            if parent_p_tag:\n",
    "                                \n",
    "                                # Find all subsequent <p> tags\n",
    "                                next_p_tags = parent_p_tag.find_next_siblings('p')\n",
    "                                #print(len(next_p_tags))\n",
    "                                # Extract the text content of each subsequent <p> tag until 'Conclusion' is found\n",
    "                                for p_tag in next_p_tags:\n",
    "                                    #print(p_tag.get_text)\n",
    "                                    # Check if the 'Conclusion'paragraph is found\n",
    "                                    if 'Conclusion' in p_tag.get_text():\n",
    "                                        break  # Do not continue extraction of the paragraph text\n",
    "                                    \n",
    "                                    # If there is no 'Conclusion' \n",
    "                                    # Check word count of current paragraph\n",
    "                                    words_count_current = len(p_tag.get_text().split())\n",
    "                                    \n",
    "                                    # Check word count of subsequent paragraphs\n",
    "                                    words_count_next = len(' '.join([tag.get_text() for tag in p_tag.find_next_siblings('p')]).split())\n",
    "                                    \n",
    "                                    # Check if current paragraph has more than 10 words and subsequent paragraphs have no more than 10 words\n",
    "                                    if words_count_current > 10 and words_count_next <= 10:\n",
    "                                        break\n",
    "                                    \n",
    "                                    #print(p_tag.get_text().strip())\n",
    "                                    extracted_text += p_tag.get_text().strip()\n",
    "\n",
    "                                break  # Once all conditions fulfilled, stop extraction\n",
    "                        \n",
    "                    if extracted_text == '': # If extracted text is still empty\n",
    "                        #print('here')\n",
    "                        # Find all paragraphs with class attribute 'Judg-Heading-x'\n",
    "                        judg_heading_paragraphs = soup.find_all('p', class_=lambda x: x and x.startswith('Judg-Heading-'))\n",
    "\n",
    "                        justify_paragraphs = soup.find_all('p', align='justify')\n",
    "                        # Iterate through each paragraph\n",
    "                        for paragraph in judg_heading_paragraphs:\n",
    "\n",
    "                            # Check if keywords of facts and issues is in the paragraph text\n",
    "                            if [identifier for identifier in fact_issues_identifiers if (identifier in text)]:\n",
    "                                \n",
    "                                # Find all subsequent <p> tags\n",
    "                                next_p_tags = paragraph.find_next_siblings('p')\n",
    "                                \n",
    "                                # Extract the text content of each subsequent <p> tag until 'Conclusion'is found\n",
    "                                for p_tag in next_p_tags:\n",
    "                                    # Check if the 'Conclusion' paragraph is found\n",
    "                                    if 'Conclusion' in p_tag.get_text():\n",
    "                                        break  # Stop extraction of the paragraph if 'Conclusion' is found\n",
    "\n",
    "                                    # Using word count as stopping criteria for paragraphs without 'Conclusion\n",
    "\n",
    "                                    # Check word cout of current paragraph\n",
    "                                    words_count_current = len(p_tag.get_text().split())\n",
    "\n",
    "                                    # Check word count of subsequent paragraphs\n",
    "                                    words_count_next = len(' '.join([tag.get_text() for tag in p_tag.find_next_siblings('p')]).split())\n",
    "\n",
    "                                    # Check if current paragraph has more than 10 words and subsequent paragraphs have no more than 10 words\n",
    "                                    if words_count_current > 10 and words_count_next <= 10:\n",
    "                                        break  # Stop iteration if condition is met\n",
    "                                    extracted_text += p_tag.get_text().strip()\n",
    "\n",
    "                    if extracted_text == '':\n",
    "                        # instance where indentifier is in a alignment justified paragraph\n",
    "                        justify_paragraphs = soup.find_all('p', align='justify')\n",
    "                        #print(justify_paragraphs)\n",
    "                        for paragraph in justify_paragraphs:\n",
    "                            if paragraph.get_text(strip=True) == 'Background':\n",
    "                                # Find all subsequent <p> tags\n",
    "                                next_p_tags = paragraph.find_all_next('p')\n",
    "                                #print(next_p_tags)\n",
    "                                # Extract the text content of each subsequent <p> tag until 'Conclusion'is found\n",
    "                                for p_tag in next_p_tags:\n",
    "                                    # Check if the 'Conclusion' paragraph is found\n",
    "                                    if 'Conclusion' in p_tag.get_text():\n",
    "                                        break  # Stop extraction of the paragraph if 'Conclusion' is found\n",
    "\n",
    "                                    # Using word count as stopping criteria for paragraphs without 'Conclusion\n",
    "\n",
    "                                    # Check word cout of current paragraph\n",
    "                                    words_count_current = len(p_tag.get_text().split())\n",
    "\n",
    "                                    # Check word count of subsequent paragraphs\n",
    "                                    words_count_next = len(' '.join([tag.get_text() for tag in p_tag.find_next_siblings('p')]).split())\n",
    "\n",
    "                                    # Check if current paragraph has more than 10 words and subsequent paragraphs have no more than 10 words\n",
    "                                    if words_count_current > 10 and words_count_next <= 10:\n",
    "                                        break  # Stop iteration if condition is met\n",
    "                                    extracted_text += p_tag.get_text().strip()\n",
    "\n",
    "                    # Append the data to the list\n",
    "                    data.append({'File Name': f'Case_{year}_{court}_{case_id}.pdf', 'text': remove_html_tags(extracted_text)})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('extracted_text_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "True     3162\n",
       "False    2323\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df2 = pd.read_csv('extracted_text_data.csv')\n",
    "df2['text'].isna().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ARCHIVE \"\"\"\n",
    "#################### SECOND VERSION ####################\n",
    "# # Send a GET request to the webpage\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Parse the HTML content of the webpage\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# # Initialize an empty string to store the extracted text\n",
    "# extracted_text = ''\n",
    "\n",
    "# # Find all <strong> tags\n",
    "# strong_tags = soup.find_all('strong')\n",
    "\n",
    "# # Iterate through all <strong> tags\n",
    "# for strong_tag in strong_tags:\n",
    "#     text = strong_tag.get_text()\n",
    "#     # Check if 'background fact' is in the text\n",
    "#     if any(keyword in text for keyword in ['Background facts', 'The facts', 'Background', 'Facts', 'Finding of Fact', \n",
    "#                                            'The issues', 'The appeal', 'Appeal', 'The issues on appeal', 'The claim', 'The present claim', \n",
    "#                                            'Background to the dispute', 'The substantive issue', 'Issues to be determined', 'The relevant issues']):\n",
    "#         # Find the parent <p> tag of the current <strong> tag\n",
    "#         parent_p_tag = strong_tag.find_parent('p')\n",
    "#         if parent_p_tag:\n",
    "#             # Find all subsequent <p> tags\n",
    "#             next_p_tags = parent_p_tag.find_next_siblings('p')\n",
    "#             # Extract the text content of each subsequent <p> tag until 'Conclusion' is found\n",
    "#             for p_tag in next_p_tags:\n",
    "#                 # Check if the 'Conclusion' paragraph is found\n",
    "#                 if 'Conclusion' in p_tag.get_text():\n",
    "#                     break  # Stop iteration if 'Conclusion' is found\n",
    "#                 # Check word count of current paragraph\n",
    "#                 words_count_current = len(p_tag.get_text().split())\n",
    "#                 # Check word count of subsequent paragraphs\n",
    "#                 words_count_next = len(' '.join([tag.get_text() for tag in p_tag.find_next_siblings('p')]).split())\n",
    "#                 # Check if current paragraph has more than 10 words and subsequent paragraphs have no more than 10 words\n",
    "#                 if words_count_current > 10 and words_count_next <= 10:\n",
    "#                     break  # Stop iteration if condition is met\n",
    "#                 extracted_text += p_tag.get_text().strip() + '\\n'\n",
    "#             break  # Stop searching once 'background fact' is found\n",
    "\n",
    "# # Find all paragraphs with class attribute 'Judg-Heading-x'\n",
    "# judg_heading_paragraphs = soup.find_all('p', class_=lambda x: x and x.startswith('Judg-Heading-'))\n",
    "\n",
    "# # Iterate through each paragraph\n",
    "# for paragraph in judg_heading_paragraphs:\n",
    "#     # Check if 'background fact' is in the paragraph text\n",
    "#     if any(keyword in paragraph.get_text() for keyword in ['Background facts', 'The facts', 'Background', 'Facts', 'Finding of Fact', \n",
    "#                                            'The issues', 'The appeal', 'Appeal', 'The issues on appeal', 'The claim', 'The present claim', \n",
    "#                                            'Background to the dispute', 'The substantive issue', 'Issues to be determined', 'The relevant issues']):\n",
    "#         # Find all subsequent <p> tags\n",
    "#         next_p_tags = paragraph.find_next_siblings('p')\n",
    "#         # Extract the text content of each subsequent <p> tag until 'Conclusion' is found\n",
    "#         for p_tag in next_p_tags:\n",
    "#             # Check if the 'Conclusion' paragraph is found\n",
    "#             if 'Conclusion' in p_tag.get_text():\n",
    "#                 break  # Stop iteration if 'Conclusion' is found\n",
    "#             # Check word count of current paragraph\n",
    "#             words_count_current = len(p_tag.get_text().split())\n",
    "#             # Check word count of subsequent paragraphs\n",
    "#             words_count_next = len(' '.join([tag.get_text() for tag in p_tag.find_next_siblings('p')]).split())\n",
    "#             # Check if current paragraph has more than 10 words and subsequent paragraphs have no more than 10 words\n",
    "#             if words_count_current > 10 and words_count_next <= 10:\n",
    "#                 break  # Stop iteration if condition is met\n",
    "#             extracted_text += p_tag.get_text().strip() + '\\n'\n",
    "\n",
    "# # Print the extracted text\n",
    "# print(index_paragraph_exclude_last_15(extracted_text))\n",
    "\n",
    "\n",
    "#################### FIRST VERSION ####################\n",
    "\n",
    "# url = 'https://www.elitigation.sg/gd/gd/2000_SGHC_5'\n",
    "\n",
    "# # Send a GET request to the webpage\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Parse the HTML content of the webpage\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# # Find all <strong> tags\n",
    "# strong_tags = soup.find_all('strong')\n",
    "# #print(strong_tags)\n",
    "\n",
    "\n",
    "\n",
    "# def index_paragraph_exclude_last_15(paragraph):\n",
    "#     # Tokenize the paragraph into sentences\n",
    "#     sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "#     # Calculate the index range to exclude the last 10 sentences\n",
    "#     start_index = 0\n",
    "#     end_index = max(0, len(sentences) - 15)\n",
    "\n",
    "#     # Reconstruct the paragraph excluding the last 10 sentences\n",
    "#     indexed_paragraph = ' '.join(sentences[start_index:end_index])\n",
    "\n",
    "#     return indexed_paragraph\n",
    "# text = ''\n",
    "# # Iterate through all <strong> tags\n",
    "# for strong_tag in strong_tags:\n",
    "#     #print(strong_tag)\n",
    "#     text = strong_tag.get_text()\n",
    "\n",
    "#     # Check if 'background fact' is in the text\n",
    "#     if 'Background facts' in text or 'The facts' in text or 'Background' in text or 'Facts' in text or 'Finding of Fact' in text:\n",
    "#         print(text)\n",
    "#     if 'Background facts' or 'The facts' or 'Background' or 'Facts' or 'Finding of Fact' in text:\n",
    "#         # Find the parent <p> tag of the current <strong> tag\n",
    "#         parent_p_tag = strong_tag.find_parent('p')\n",
    "#         if parent_p_tag:\n",
    "#             # Find all subsequent <p> tags\n",
    "#             next_p_tags = parent_p_tag.find_next_siblings('p')\n",
    "#             #print(len(next_p_tags))\n",
    "#             # Extract and print the text content of each subsequent <p> tag until 'Conclusion' is found\n",
    "#             for p_tag in next_p_tags:\n",
    "#                 # Check if the 'Conclusion' paragraph is found\n",
    "#                 if 'Conclusion' in p_tag.get_text():\n",
    "#                     break  # Stop iteration if 'Conclusion' is found\n",
    "#                 text += p_tag.get_text().strip()\n",
    "#             break  # Stop searching once 'background fact' is found\n",
    "\n",
    "#print(remove_html_tags(index_paragraph_exclude_last_15(text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
